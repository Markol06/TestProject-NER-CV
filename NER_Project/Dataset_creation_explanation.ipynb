{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76af24da-c1ff-44a2-b827-79009e05a390",
   "metadata": {},
   "source": [
    "So, at first, I decided to use GPT for data generation. It generates data quite well, but not in large quantities, which makes the process time-consuming.\n",
    "Initially, I generated the first 100 sentences for testing.\n",
    "Next, I searched on Kaggle and found a dataset with mountain names, coordinates, and other data. I decided to use this dataset for my work.\n",
    "First, I loaded the dataset and displayed the number of unique mountain names it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc1ef8-7fbf-408b-90d8-54e048002766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload file\n",
    "df = pd.read_csv('Mountain.csv')\n",
    "\n",
    "# Counting the number of unique mountain names\n",
    "num_mountains = df['Mountain'].nunique()\n",
    "print(f\"Number of unique mountain names: {num_mountains}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3d72b-7cbd-4035-bff0-7aca7cbd447b",
   "metadata": {},
   "source": [
    "Number of unique mountain names: 1621\n",
    "I decided that it would be better to train the model on real data rather than GPT-generated data.\n",
    "I thought about where I could find texts about a large number of mountains and extract them, and I settled on Wikipedia.\n",
    "I wrote a script that allows extracting the required amount of mountain data using the Wikipedia API.\n",
    "In this script, you can adjust the number of mountains you want to extract—I started with 400 and eventually settled on 1200.\n",
    "You can also control how many characters you want to extract from an article about a specific mountain—I started with 2000 and eventually settled on 500.\n",
    "Increasing the dataset positively affected the result, and typically, the essential information about a mountain is presented in the first sentences of the text, so I decided to extract only 500 characters (although I think it could have been even less).\n",
    "It would also be worth improving the naming since manually changing the variable name every time based on the number of mountains extracted is not very convenient.\n",
    "For example: mountain_names_600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb9b7e-4f80-4037-bb7b-26dc0df01e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wikipedia API with User-Agent\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    user_agent=\"Mozilla/5.0 (compatible; MyNLPApp/1.0; +https://example.com)\"\n",
    ")\n",
    "\n",
    "# Path to the mountain file\n",
    "file_path = 'Mountain.csv'\n",
    "\n",
    "# Loading data from a CSV file\n",
    "df_mountains = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the first 1200 mountain names from the 'Mountain' column\n",
    "mountain_names_600 = df_mountains['Mountain'].head(1200).tolist()\n",
    "\n",
    "# Виведемо кількість витягнутих назв гір для перевірки\n",
    "print(f\"Number of mountains drawn: {len(mountain_names_600)}\")\n",
    "print(f\"Top 10 mountains: {mountain_names_600[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7667cec-7a1f-4fb2-8d91-1f979420af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for downloading article texts\n",
    "def fetch_mountain_texts(mountain_names):\n",
    "    texts = {}\n",
    "    for name in mountain_names:\n",
    "        page = wiki_wiki.page(name)\n",
    "        if page.exists():\n",
    "            texts[name] = page.text[:500]  # Limit the text to the first 500 characters\n",
    "    return texts\n",
    "\n",
    "# Loading texts\n",
    "mountain_texts_600 = fetch_mountain_texts(mountain_names_600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614c4b7-01ce-4be2-9237-0a471fe96577",
   "metadata": {},
   "source": [
    "I displayed the text to evaluate it visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71ebd9-0cf4-4c8d-a72a-b90c60a88b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a preview for multiple mountains\n",
    "mountain_texts_preview_600 = {name: text[:300] for name, text in mountain_texts_600.items()}\n",
    "len(mountain_texts_600), mountain_texts_preview_600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd9e82-b4e1-450f-9ab8-200ddd26f81e",
   "metadata": {},
   "source": [
    "Next, I decided to determine the \"main\" mountain in a Wikipedia article, as various mountains are usually mentioned in a single article (due to comparisons, etc.). Initially, I thought this would be helpful, but after forming and testing the model on the new data, I started to believe that it might have been a mistake, and I shouldn't have done it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072800d-3821-42ae-81d0-a1105476a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the main mountain in the text\n",
    "def get_main_mountain(text, mountain_names):\n",
    "    mountain_counts = Counter()\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    for mountain in mountain_names:\n",
    "        count = text_lower.count(mountain)  # Count the number of mentions of each mountain\n",
    "        if count > 0:\n",
    "            mountain_counts[mountain] = count\n",
    "            \n",
    "    if mountain_counts:\n",
    "        main_mountain = mountain_counts.most_common(1)[0][0]  # Picking the mountain with the most mentions\n",
    "        return main_mountain\n",
    "    return None\n",
    "\n",
    "# Function for marking up texts taking into account the main mountain\n",
    "def annotate_mountains_by_main(texts, mountain_names):\n",
    "    annotated_data = []\n",
    "    for name, text in texts.items():\n",
    "        main_mountain = get_main_mountain(text, mountain_names)  # Determine the main mountain\n",
    "        if main_mountain:\n",
    "            sentences = re.split(r'(?<=[.!?]) +', text)  # Split the text into sentences\n",
    "            for sentence in sentences:\n",
    "                found_mountains = [mountain for mountain in mountain_names if mountain in sentence.lower()]\n",
    "                if found_mountains:\n",
    "                    annotated_data.append({\n",
    "                        'sentence': sentence,\n",
    "                        'main_mountain': main_mountain,\n",
    "                        'mentioned_mountains': found_mountains\n",
    "                    })\n",
    "    return annotated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edef111c-e45b-4dca-9de7-dc712a0b295c",
   "metadata": {},
   "source": [
    "Next, I decided to add synthetic data (generated by GPT) to improve the model’s performance. Therefore, I kept the existing dataset and displayed the current number of sentences in it:\n",
    "Number of sentences in the dataset before adding synthetic data: 1958."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7dfe3-6f0a-4474-b632-c0bfa2a90108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text markup\n",
    "annotated_dataset_by_main = annotate_mountains_by_main(mountain_texts_600, mountain_names_lower)\n",
    "\n",
    "# Convert to DataFrame for saving\n",
    "df_annotated_main = pd.DataFrame(annotated_dataset_by_main)\n",
    "\n",
    "# Output the number of sentences in the dataset\n",
    "print(f\"Number of sentences in the dataset before adding synthetic data: {df_annotated_main.shape[0]}\")\n",
    "\n",
    "# Saving the dataset in CSV format\n",
    "output_path = 'annotated_mountain_dataset.csv'\n",
    "df_annotated_main.to_csv(output_path, index=False)\n",
    "\n",
    "# Show the first few lines\n",
    "df_annotated_main.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2afd4-fe51-4b69-8fea-25d1eda7d9bf",
   "metadata": {},
   "source": [
    "I generated synthetic data using GPT (around 1000 sentences), loaded the data, processed it (ignoring insertion markers), mixed it with the Wikipedia data, and saved the updated dataset.\n",
    "Combined dataset has been saved to 'combined_annotated_dataset.csv'\n",
    "Size of the merged dataset: (2992, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4acb71-38b9-4ff4-a305-db97b0d68882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading synthetic data\n",
    "df_synthetic = pd.read_csv('synthetic_data.csv', header=None, names=['sentence'])\n",
    "\n",
    "# Filter the lines to exclude those containing only \"[\" or \"]\"\n",
    "df_synthetic = df_synthetic[~df_synthetic['sentence'].str.contains(r'^\\[|\\]$', regex=True)]\n",
    "\n",
    "# Create a column 'main_mountain' for synthetic data\n",
    "df_synthetic['main_mountain'] = df_synthetic['sentence'].apply(lambda x: x.split()[1] if len(x.split()) > 1 else x)\n",
    "\n",
    "# Combine the main dataset with synthetic data\n",
    "df_combined = pd.concat([df_annotated_main, df_synthetic], ignore_index=True)\n",
    "\n",
    "# Fill NaN values in 'mentioned_mountains' with empty lists\n",
    "df_combined['mentioned_mountains'] = df_combined['mentioned_mountains'].fillna('[]')\n",
    "\n",
    "# Save the combined dataset to a new CSV file\n",
    "df_combined.to_csv('combined_annotated_dataset.csv', index=False)\n",
    "print(\"Combined dataset has been saved to 'combined_annotated_dataset.csv'\")\n",
    "\n",
    "# Output the size of the new merged dataset for verification\n",
    "print(f\"Size of the merged dataset: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f3b06-8bf4-49a4-923e-6fedc60718e4",
   "metadata": {},
   "source": [
    "Some mountain names consist of multiple words, so I wrote a function to split such names into separate tokens. As a result, three tokens were defined:\n",
    "B-Mountain – the beginning of a mountain name,\n",
    "I-Mountain – the continuation of a mountain name,\n",
    "O – all other tokens.\n",
    "I also believe that improving this function could have enhanced the model's performance. It would have been possible to better handle mountain-related tokens—for example, if the model identifies a word as part of a mountain name continuation, it should pay more attention to ensuring that the beginning of the mountain name appears somewhere as well, or perhaps try to completely revise the logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72665d-dce3-4d5f-aac5-1091e22a993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ner_dataset_fixed_multiword(df, mountain_names):\n",
    "    \"\"\"\n",
    "    Prepare a dataset for training a NER model taking into account multi-word mountain names.\n",
    "\n",
    "    Returns a list of sentences tokenized into words and the corresponding labels.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence'].split()  # tokenization\n",
    "        main_mountain = row['main_mountain'].lower()  # Main mountain in the article\n",
    "        \n",
    "        sentence_labels = ['O'] * len(sentence)  # Initialize the label list with the value 'O'\n",
    "        \n",
    "        # Check all the long-form mountain names in the sentence\n",
    "        for mountain in mountain_names:\n",
    "            mountain_tokens = mountain.split()  # Break the name of the mountain into words\n",
    "            mountain_len = len(mountain_tokens)\n",
    "            \n",
    "            # Check each substring in the sentence\n",
    "            for i in range(len(sentence) - mountain_len + 1):\n",
    "                window = sentence[i:i + mountain_len]  # A substring of a sentence as long as the name of a mountain\n",
    "                window_lower = [word.lower().strip(string.punctuation) for word in window]\n",
    "                \n",
    "                if window_lower == mountain_tokens:\n",
    "                    sentence_labels[i] = 'B-MOUNTAIN'  # Beginning of mountain name\n",
    "                    for j in range(1, mountain_len):\n",
    "                        sentence_labels[i + j] = 'I-MOUNTAIN'  # Continuation of the mountain name\n",
    "        \n",
    "        sentences.append(sentence)\n",
    "        labels.append(sentence_labels)\n",
    "    \n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7a4c2c-7121-4eca-a34e-cc9d37bbcccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "sentences_fixed_multiword, labels_fixed_multiword = prepare_ner_dataset_fixed_multiword(df_combined, mountain_names_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ed628-7cbc-473c-942b-af3cc8dd8d7f",
   "metadata": {},
   "source": [
    "Next, I decided to remove stop words. This was primarily necessary to address the main issue – poor balance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfdd512-6059-4da8-b270-f7be95115e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's output an example sentence before removing stop words\n",
    "print(\"Before removing stop words:\")\n",
    "print(list(zip(sentences_fixed_multiword[10], labels_fixed_multiword[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8315225-bf92-494a-aa6d-fa824bffa86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words and remove them from the dataset\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(sentences, labels):\n",
    "    \"\"\"\n",
    "    Removes stop words for class O tokens only.\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    new_labels = []\n",
    "    \n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        reduced_sentence = []\n",
    "        reduced_label = []\n",
    "        \n",
    "        for word, l in zip(sentence, label):\n",
    "            if l == 'O' and word.lower() in stop_words:\n",
    "                continue  # Skip the token if it is a class O stop word\n",
    "            reduced_sentence.append(word)\n",
    "            reduced_label.append(l)\n",
    "        \n",
    "        new_sentences.append(reduced_sentence)\n",
    "        new_labels.append(reduced_label)\n",
    "    \n",
    "    return new_sentences, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a766c-a5b1-4cce-82e2-a5c2af006ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the dataset\n",
    "sentences_reduced, labels_reduced = remove_stopwords(sentences_fixed_multiword, labels_fixed_multiword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ad489-1cb7-4e8c-a5b6-1ec0dedd57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's output an example sentence after removing stop words\n",
    "print(\"\\nAfter removing stop words:\")\n",
    "print(list(zip(sentences_reduced[10], labels_reduced[10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761853cf-a4c2-4d9d-862b-d244c73fa9f9",
   "metadata": {},
   "source": [
    "Next, I considered what else could be done to improve the balance and concluded that not all of the extracted sentences necessarily contain mentions of mountain names.\n",
    "Therefore, I decided that if a sentence does not mention a mountain, it has less impact on the model's training, and we can significantly reduce the number of O tokens by removing such sentences.\n",
    "Number of sentences after filtering: 2700."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec62dcb-71e5-4b97-84b5-12dc425c3600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences_no_mountains(sentences, labels):\n",
    "    \"\"\"\n",
    "    Removes sentences where all tokens have the label 'O' (i.e. no mention of mountain names).\n",
    "    \"\"\"\n",
    "    filtered_sentences = []\n",
    "    filtered_labels = []\n",
    "    \n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        if any(l != 'O' for l in label):  # Check if there is at least one label other than 'O'\n",
    "            filtered_sentences.append(sentence)\n",
    "            filtered_labels.append(label)\n",
    "    \n",
    "    return filtered_sentences, filtered_labels\n",
    "\n",
    "# Apply the function to the reduced dataset\n",
    "filtered_sentences, filtered_labels = filter_sentences_no_mountains(sentences_reduced, labels_reduced)\n",
    "\n",
    "# Output the number of sentences after filtering\n",
    "print(f\"Number of sentences after filtering: {len(filtered_sentences)}\")\n",
    "\n",
    "# Example sentence after filtering\n",
    "print(\"Example sentence after filtering:\")\n",
    "print(list(zip(filtered_sentences[0], filtered_labels[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc2bd5-38fb-4cae-a53f-877b01a94a6d",
   "metadata": {},
   "source": [
    "This slightly improved the situation, but the imbalance still remained. So, I decided to take a risk and randomly reduce the number of O tokens in each sentence. This was a risky move, as it could potentially remove tokens that the model might need. I chose to remove half of the O tokens to achieve a 70/30 balance, which is the minimally desirable result.\n",
    "\n",
    "As a result, the model's performance improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c370669-7bf9-4a69-b9ab-6675c24fd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_o_tokens(sentences, labels, reduction_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Selectively reduces the number of class O tokens in sentences.\n",
    "\n",
    "    Parameters:\n",
    "    - sentences: list of sentences (tokenized into words)\n",
    "    - labels: list of labels for each sentence\n",
    "    - reduction_ratio: fraction of class O tokens to remove\n",
    "\n",
    "    Returns new lists of sentences and labels.\n",
    "    \"\"\"\n",
    "    new_sentences = []\n",
    "    new_labels = []\n",
    "    \n",
    "    for sentence, label in zip(sentences, labels):\n",
    "        o_indices = [i for i, l in enumerate(label) if l == 'O']\n",
    "        num_to_remove = int(len(o_indices) * reduction_ratio)\n",
    "        \n",
    "        if num_to_remove > 0:\n",
    "            indices_to_remove = set(random.sample(o_indices, num_to_remove))\n",
    "        else:\n",
    "            indices_to_remove = set() \n",
    "        \n",
    "        reduced_sentence = [word for i, word in enumerate(sentence) if i not in indices_to_remove]\n",
    "        reduced_label = [l for i, l in enumerate(label) if i not in indices_to_remove]\n",
    "        \n",
    "        new_sentences.append(reduced_sentence)\n",
    "        new_labels.append(reduced_label)\n",
    "    \n",
    "    return new_sentences, new_labels\n",
    "\n",
    "# Apply the function to the dataset\n",
    "final_sentences_reduced, final_labels_reduced = reduce_o_tokens(filtered_sentences, filtered_labels, reduction_ratio=0.5)\n",
    "\n",
    "print(f\"Number of sentences after reducing class O tokens: {len(final_sentences_reduced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b704f-958d-4b9f-ae7d-d41d7a2e0e87",
   "metadata": {},
   "source": [
    "Next, I formed the final dataset after all the manipulations and proceeded with training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf890e6-faae-44b6-8ed9-af2656f8d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine words and tags into strings\n",
    "final_sentences_str = [' '.join(sentence) for sentence in final_sentences_reduced]\n",
    "final_labels_str = [' '.join(label) for label in final_labels_reduced]\n",
    "\n",
    "# Creating DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'sentence': final_sentences_str,\n",
    "    'label': final_labels_str\n",
    "})\n",
    "\n",
    "# Saving DataFrame in CSV file\n",
    "df.to_csv('final_dataset.csv', index=False)\n",
    "\n",
    "print(\"File successfully saved as 'final_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c622fd-0e68-4513-8019-f59679db17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading dataset\n",
    "df = pd.read_csv('final_dataset.csv')\n",
    "\n",
    "# Split the strings back into token lists\n",
    "final_sentences = [sentence.split() for sentence in df['sentence']]\n",
    "final_labels = [label.split() for label in df['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e694a-ee27-4c1f-8108-5c1054d00f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
